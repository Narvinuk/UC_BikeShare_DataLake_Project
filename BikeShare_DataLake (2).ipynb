{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4714ded0-6c66-4c5f-8b35-94902b7e35ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.functions import isnan, count, when, col, desc, udf, col, sort_array, asc, avg\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05e65d29-345d-4126-8e74-bc93c55eb2d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Creat Spark Session\n",
    "spark=SparkSession.builder.appName(\"DW_BikeShare\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2e6895a-548f-4e9b-82c4-d50a34177a20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create Filestore paths\n",
    "payments_file_path=\"/FileStore/payments.csv\"\n",
    "rider_file_path=\"/FileStore/riders.csv\"\n",
    "stations_file_path=\"/FileStore/stations.csv\"\n",
    "trips_file_path=\"/FileStore/trips.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61a30b87-44d1-4be4-aab5-830b64b84e48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create payment dataframe from csv file\n",
    "df_pay=spark.read.csv(payments_file_path,header=False,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38073810-d191-4495-9b15-4d3d3a2af6d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create rider dataframe from csv file\n",
    "df_rider=spark.read.csv(rider_file_path,header=False,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3496e72d-9998-4ad5-9a24-7fb77dd3e375",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create stations dataframe from csv file\n",
    "df_stations=spark.read.csv(stations_file_path,header=False,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd080792-e14f-4af1-8ddc-151a4bf406d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create trips dataframe from csv file\n",
    "df_trips=spark.read.csv(trips_file_path,header=False,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b16eb2cc-99df-423a-937a-0f18e7e66cd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Add header record to the dataframe\n",
    "df_riders=df_rider.toDF(\"rider_id\",\"first\",\"last\",\"address\",\"birthday\",\"account_start_date\",\"account_end_date\",\"is_member\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e85bd249-f106-457b-b6fd-b63acaf45c0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- rider_id: integer (nullable = true)\n |-- first: string (nullable = true)\n |-- last: string (nullable = true)\n |-- address: string (nullable = true)\n |-- birthday: date (nullable = true)\n |-- account_start_date: date (nullable = true)\n |-- account_end_date: date (nullable = true)\n |-- is_member: boolean (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_riders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10f43812-adcd-4b8b-9959-3a5d20d7f142",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[rider_id: int, first: string, last: string, address: string, birthday: date, account_start_date: date, account_end_date: date, is_member: boolean]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_riders.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43ee54bd-b8a3-4490-8161-7de158726f17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Add header record to the dataframe\n",
    "df_trip=df_trips.toDF(\"trip_id\",\"rideable_type\",\"started_at\",\"ended_at\",\"start_station_id\",\"end_station_id\",\"rider_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf420c38-d841-4c4a-8d63-71e46fb3fbbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- trip_id: string (nullable = true)\n |-- rideable_type: string (nullable = true)\n |-- started_at: timestamp (nullable = true)\n |-- ended_at: timestamp (nullable = true)\n |-- start_station_id: string (nullable = true)\n |-- end_station_id: string (nullable = true)\n |-- rider_id: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_trip.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20f052a9-3947-4683-baa5-0cdf1b20e6a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[trip_id: string, rideable_type: string, started_at: timestamp, ended_at: timestamp, start_station_id: string, end_station_id: string, rider_id: int]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trip.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9732d1c9-6130-4958-9605-417c649b45a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Add header record to the dataframe\n",
    "df_station=df_stations.toDF(\"station_id\",\"name\",\"latitude\",\"longitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fb5c178-3cc2-4d41-8eff-adca2f348344",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[station_id: string, name: string, latitude: double, longitude: double]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop Duplicates\n",
    "df_station.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da56091-02ed-4f99-afe7-d999f2dbabb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- station_id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_station.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c610afb-c349-4f83-8360-227b4548c2d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Add header record to the dataframe\n",
    "df_payment=df_pay.toDF(\"id\",\"date\",\"amount\",\"rider_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d3198f2-ef51-41f6-a668-0bb03b40182c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+--------+\n| id|      date|amount|rider_id|\n+---+----------+------+--------+\n|  1|2019-05-01|   9.0|    1000|\n|  2|2019-06-01|   9.0|    1000|\n|  3|2019-07-01|   9.0|    1000|\n|  4|2019-08-01|   9.0|    1000|\n|  5|2019-09-01|   9.0|    1000|\n+---+----------+------+--------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_payment.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69e155d6-c624-40b8-b63c-64f981131cdc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[id: int, date: date, amount: double, rider_id: int]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_payment.printSchema()\n",
    "df_payment.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3c4bceb-bed1-4a71-9814-94036e1ea532",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df_payment.count()\n",
    "#1946607\n",
    "#Create Delta file for Payment\n",
    "df_payment.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/bronze_payment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1024b3d4-7584-462b-93e7-e06be299fc73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+--------+\n| id|      date|amount|rider_id|\n+---+----------+------+--------+\n|  1|2019-05-01|   9.0|    1000|\n|  2|2019-06-01|   9.0|    1000|\n|  3|2019-07-01|   9.0|    1000|\n|  4|2019-08-01|   9.0|    1000|\n|  5|2019-09-01|   9.0|    1000|\n+---+----------+------+--------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_payment.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c45386f5-1b97-4621-8277-0e3b2fc1f5e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_p=spark.read.format(\"delta\").load(\"/delta/bronze_payment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3892ffcc-f4f1-4636-a281-a0c7abf75012",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(id=539256, date=datetime.date(2020, 8, 1), amount=9.0, rider_id=21826),\n",
       " Row(id=539257, date=datetime.date(2020, 9, 1), amount=9.0, rider_id=21826),\n",
       " Row(id=539258, date=datetime.date(2020, 10, 1), amount=9.0, rider_id=21826),\n",
       " Row(id=539259, date=datetime.date(2020, 11, 1), amount=9.0, rider_id=21826),\n",
       " Row(id=539260, date=datetime.date(2020, 12, 1), amount=9.0, rider_id=21826)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dd19512-6921-46e8-aa40-671ca800aa71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Create Delta table for Payment\n",
    "spark.sql (\"CREATE TABLE IF NOT EXISTS silver_payment USING DELTA LOCATION '/delta/bronze_payment' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "211beb42-30db-4049-ba16-b5103af6da40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Delta table for rider\n",
    "df_riders.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/bronze_rider\")\n",
    "spark.sql (\"CREATE TABLE IF NOT EXISTS silver_rider USING DELTA LOCATION '/delta/bronze_rider' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c3a24de-0dfa-472e-9cad-ce9ccc0f9153",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Delta table for trips data\n",
    "df_trip.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/bronze_trip\")\n",
    "spark.sql (\"CREATE TABLE IF NOT EXISTS silver_trip USING DELTA LOCATION '/delta/bronze_trip' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1062b9f7-d31e-486d-90f9-77cedfeeb903",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Delta table for Stations data\n",
    "df_station.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/bronze_station\")\n",
    "spark.sql (\"CREATE TABLE IF NOT EXISTS silver_station USING DELTA LOCATION '/delta/bronze_station' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a276b3e-cfae-4b80-b65d-c49bcb02c22b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Payment Fact table \n",
    "spark.sql('''CREATE TABLE IF NOT EXISTS payment_fact USING DELTA AS \n",
    "          SELECT DISTINCT id AS payment_id, date,amount,rider_id FROM silver_payment''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7b0c878-b0f9-46a9-b797-3c62c7f43f49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------+--------+\n|payment_id|      date|amount|rider_id|\n+----------+----------+------+--------+\n|       165|2019-02-01| 15.47|    1007|\n|       342|2019-09-01|   9.0|    1014|\n+----------+----------+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select * from payment_fact limit 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50e73758-8f96-4f26-809b-a224fc9a7461",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''CREATE TABLE IF NOT EXISTS dim_rider USING DELTA AS \n",
    "          SELECT DISTINCT * FROM silver_rider''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30f92093-d52b-477f-b26c-5de0eced020c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+---------+--------------------+----------+------------------+----------------+---------+\n|rider_id|  first|     last|             address|  birthday|account_start_date|account_end_date|is_member|\n+--------+-------+---------+--------------------+----------+------------------+----------------+---------+\n|    1040|Matthew|    Watts|   323 Matthew Flats|1996-12-24|        2014-11-26|            NULL|     true|\n|    1065|Rebecca|Mccormick|4912 Smith Alley ...|1983-07-18|        2017-07-13|            NULL|    false|\n+--------+-------+---------+--------------------+----------+------------------+----------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from dim_rider  limit 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c8948cc-0069-4a86-a1d6-682fbdec942b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''CREATE TABLE IF NOT EXISTS dim_station USING DELTA AS \n",
    "          SELECT DISTINCT  *  FROM silver_station''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "348c4960-38cd-4ef2-abdb-60da15affce7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------------+------------------+\n|  station_id|                name|         latitude|         longitude|\n+------------+--------------------+-----------------+------------------+\n|       15550|Canal St & Taylor St|        41.870257|-87.63947399999999|\n|TA1307000150|Pine Grove Ave & ...|41.94947274088333|-87.64645278453827|\n+------------+--------------------+-----------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from dim_station limit 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a81fea09-6685-4a90-b40a-463fff80227a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Dim Ridetype table\n",
    "spark.sql ('''\n",
    "           CREATE TABLE IF NOT EXISTS dim_ride_type\n",
    "USING DELTA\n",
    "AS\n",
    "SELECT\n",
    "  ROW_NUMBER() OVER (ORDER BY rideable_type) AS ride_type_id,\n",
    "  rideable_type\n",
    "FROM\n",
    "  silver_trip\n",
    "GROUP BY\n",
    "  rideable_type\n",
    "  '''\n",
    "           \n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bd347a6-34c8-4249-b725-60374506518c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n|ride_type_id|rideable_type|\n+------------+-------------+\n|           1| classic_bike|\n|           2|  docked_bike|\n|           3|electric_bike|\n+------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql (\"select * from dim_ride_type\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a7273eb-ec33-452f-8237-49bfd77f4472",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------+--------+\n|payment_id|      date|amount|rider_id|\n+----------+----------+------+--------+\n|    856454|2021-01-01|   9.0|   34062|\n|    856453|2020-12-01|   9.0|   34062|\n+----------+----------+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql (\"select * from payment_fact where rider_id=34062\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "615e34e0-43f4-4154-a940-c485c2bbf635",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------+--------+\n|payment_id|      date|amount|rider_id|\n+----------+----------+------+--------+\n|       165|2019-02-01| 15.47|    1007|\n|       342|2019-09-01|   9.0|    1014|\n|       398|2019-10-01|   9.0|    1015|\n|       543|2020-04-01|   9.0|    1022|\n|       554|2021-03-01|   9.0|    1022|\n+----------+----------+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql (\"SELECT DISTINCT id AS payment_id, date,amount,rider_id FROM silver_payment limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "140bbc2e-4c04-40f3-a15b-3b7ad6177278",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n|rider_id|Trip_Counts|\n+--------+-----------+\n|   53044|       1636|\n|   37388|       1584|\n|   21973|       1537|\n|   33748|       1529|\n|   14363|       1480|\n|   61581|       1433|\n|    5211|       1422|\n|   66814|       1398|\n|   67878|       1382|\n|    4193|       1367|\n|   71748|       1366|\n|   19330|       1361|\n|   39417|       1355|\n|   52113|       1346|\n|   43849|       1346|\n|   25890|       1345|\n|   50994|       1343|\n|   55633|       1339|\n|   55955|       1335|\n|    6048|       1333|\n+--------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Trip Counts by rider in descending order.\n",
    "spark.sql ('''select r.rider_id,count(distinct trip_id)  as Trip_Counts from silver_trip t join silver_rider r on t.rider_id=r.rider_id \n",
    "           group by r.rider_id\n",
    "           Order by count(distinct trip_id) desc\n",
    "            ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "641cf51a-e675-491b-90f5-7a5ea3c76cbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+---+-----------+\n|rider_id|         trip_id|age|trip_amount|\n+--------+----------------+---+-----------+\n|    1000|794BFB4727EFB58B| 32|      306.0|\n|    1000|D3BE89A24107DA70| 32|      306.0|\n|    1000|E2C779C18C7152A9| 32|      306.0|\n|    1000|632A33F2C25D5990| 32|      306.0|\n|    1000|957978AA49C230A5| 32|      306.0|\n|    1000|24FB5B82406EB7B8| 32|      306.0|\n|    1000|DA619FAE456C953E| 32|      306.0|\n|    1000|3617865036B0468A| 32|      306.0|\n|    1000|F0C6DD1F7754C733| 32|      306.0|\n|    1000|B7CFA527619455F5| 32|      306.0|\n|    1000|B37B922E4A2087E9| 32|      306.0|\n|    1000|D8CDC7E28EF69BB7| 32|      306.0|\n|    1000|A44F47CF28EE6F6B| 32|      306.0|\n|    1000|2BFCC9CA079188B2| 32|      306.0|\n|    1000|4EBB0B75CACB886C| 32|      306.0|\n|    1000|0FC0422D68F59E02| 32|      306.0|\n|    1000|9576415E314CD525| 32|      306.0|\n|    1003|5D1E06F75542A21C| 22|     411.07|\n|    1003|E69CF13359BB0177| 22|     411.07|\n|    1003|5FB921A23BCC2A09| 22|     411.07|\n+--------+----------------+---+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql ('''select r.rider_id, t.trip_id,FLOOR(DATEDIFF(t.started_at, r.birthday) / 365.25) AS age \n",
    "           ,sum(amount) as trip_amount\n",
    "           from silver_trip t join silver_rider r on t.rider_id=r.rider_id \n",
    "join silver_payment p on p.rider_id=r.rider_id           \n",
    "Group By r.rider_id, t.trip_id,FLOOR(DATEDIFF(t.started_at, r.birthday) / 365.25)\n",
    "Order by rider_id\n",
    "           ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40339964-f52f-4182-86c9-fd53522c515e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------+\n|rider_id|count(DISTINCT payment_id)|\n+--------+--------------------------+\n|   18979|                        49|\n|    9427|                        21|\n|   13289|                        68|\n|   18866|                        52|\n|    3175|                        11|\n|    3749|                         7|\n|   38723|                        45|\n|   35361|                        38|\n|   33722|                         6|\n|   37307|                        36|\n|   26706|                        11|\n|   41890|                        88|\n|   44358|                        73|\n|   46943|                        24|\n|   53963|                        49|\n|   43103|                        96|\n|    7240|                        23|\n|   41751|                        53|\n|   28836|                        68|\n|   55283|                        20|\n+--------+--------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql (\"select rider_id,count(distinct payment_id) from payment_fact group by rider_id having count(distinct payment_id) > 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32a8653d-b5c9-4b58-b51c-2131ff98d7d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql ('''\n",
    "           CREATE TABLE IF NOT EXISTS trip_fact USING DELTA AS \n",
    "\n",
    "           select t.trip_id ,t.rider_id,t.start_station_id,t.end_station_id,rt.ride_type_id,started_at,ended_at,\n",
    "           round(datediff (second, t.started_at, t.ended_at) / 60)  AS trip_duration,\n",
    "            floor (months_between (t.started_at, r.birthday) / 12) as rider_age,r.is_member\n",
    "from silver_trip t\n",
    "join dim_rider r on t.rider_id=r.rider_id\n",
    "join dim_station s on s.station_id=t.start_station_id\n",
    "join dim_station se on se.station_id=t.end_station_id\n",
    "join dim_ride_type rt on rt.rideable_type=t.rideable_type\n",
    "\n",
    "'''\n",
    "           \n",
    "           \n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e4044bd-87e6-400e-8c2a-951c51061227",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+----------------+--------------+------------+-------------------+-------------------+-------------+---------+---------+\n|         trip_id|rider_id|start_station_id|end_station_id|ride_type_id|         started_at|           ended_at|trip_duration|rider_age|is_member|\n+----------------+--------+----------------+--------------+------------+-------------------+-------------------+-------------+---------+---------+\n|7E1E50AC37E2DAD3|    2644|    TA1309000007|         13089|           1|2021-08-14 14:01:36|2021-08-14 14:34:49|         33.0|       45|     true|\n|ADFF32195521E952|   37747|           13288|  TA1308000031|           1|2021-08-29 16:16:36|2021-08-29 16:24:43|          8.0|       19|     true|\n+----------------+--------+----------------+--------------+------------+-------------------+-------------------+-------------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql (\"select * from trip_fact limit 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89ccb1fb-2bd7-49d0-b64d-5ed2fef446b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n|           min_date|           max_Date|\n+-------------------+-------------------+\n|2021-02-01 01:07:04|2022-02-01 00:12:04|\n+-------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql (\"select min(started_at) as min_date,max(ended_at) as max_Date from trip_fact limit 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8bd4bae-cc1e-4336-85fc-1832c6c1adde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F \n",
    "min_date=spark.sql(\"select date(min(started_at)) from trip_fact\").collect()[0][0]\n",
    "#print(min_date)\n",
    "max_date=spark.sql(\"select date(max(started_at)) from trip_fact\").collect()[0][0]\n",
    "#print(max_date)\n",
    "date_dim = spark.range(0, (max_date - min_date).days + 1) \\\n",
    "    .withColumn(\"date\", F.date_add(F.lit(min_date), F.col(\"id\").cast(\"integer\"))) \\\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"date\").cast(\"integer\")) \\\n",
    "    .withColumn(\"day_of_month\", F.dayofmonth(\"date\").cast(\"integer\")) \\\n",
    "    .withColumn(\"day_of_year\", F.dayofyear(\"date\").cast(\"integer\")) \\\n",
    "    .withColumn(\"week_of_year\", F.weekofyear(\"date\").cast(\"integer\")) \\\n",
    "    .withColumn(\"month\", F.month(\"date\").cast(\"integer\")) \\\n",
    "    .withColumn(\"month_name\", F.monthname(\"date\")) \\\n",
    "    .withColumn(\"quarter\", F.quarter(\"date\").cast(\"integer\")) \\\n",
    "    .withColumn(\"year\", F.year(\"date\").cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb7f1c44-2067-4dde-be84-f20f89415d8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------------+-----------+------------+-----+----------+-------+----+\n| id|      date|day_of_week|day_of_month|day_of_year|week_of_year|month|month_name|quarter|year|\n+---+----------+-----------+------------+-----------+------------+-----+----------+-------+----+\n|  0|2021-02-01|          2|           1|         32|           5|    2|       Feb|      1|2021|\n|  1|2021-02-02|          3|           2|         33|           5|    2|       Feb|      1|2021|\n|  2|2021-02-03|          4|           3|         34|           5|    2|       Feb|      1|2021|\n|  3|2021-02-04|          5|           4|         35|           5|    2|       Feb|      1|2021|\n|  4|2021-02-05|          6|           5|         36|           5|    2|       Feb|      1|2021|\n+---+----------+-----------+------------+-----------+------------+-----+----------+-------+----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "date_dim.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ac8bfe8-9848-4fd4-b807-f4267990eba8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "date_dim.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"date_dim\")\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96027540-f5c2-4df5-991b-cc8dae3d6188",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------------+-----------+------------+-----+----------+-------+----+\n| id|      date|day_of_week|day_of_month|day_of_year|week_of_year|month|month_name|quarter|year|\n+---+----------+-----------+------------+-----------+------------+-----+----------+-------+----+\n|273|2021-11-01|          2|           1|        305|          44|   11|       Nov|      4|2021|\n|274|2021-11-02|          3|           2|        306|          44|   11|       Nov|      4|2021|\n|275|2021-11-03|          4|           3|        307|          44|   11|       Nov|      4|2021|\n|276|2021-11-04|          5|           4|        308|          44|   11|       Nov|      4|2021|\n|277|2021-11-05|          6|           5|        309|          44|   11|       Nov|      4|2021|\n+---+----------+-----------+------------+-----------+------------+-----+----------+-------+----+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from date_dim limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a89abdb1-1cff-452e-9599-1880c2dab11e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Data Anaylysis  how much time is spent per ride Based on the following Factors:\n",
    "- Based on date and time factors such as day of week and time of day\n",
    "- Based on which station is the starting and / or ending station\n",
    "- Based on age of the rider at time of the ride\n",
    "- Based on whether the rider is a member or a casual rider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f2670b1-d639-429b-836f-3e0abeff6dd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-------+-----+------------+----------+\n|rider_id|year|quarter|month|Total_Amount|Avg_Amount|\n+--------+----+-------+-----+------------+----------+\n|    1000|2021|      1|    2|         9.0|       9.0|\n|    1000|2021|      1|    3|         9.0|       9.0|\n|    1000|2021|      2|    4|         9.0|       9.0|\n|    1000|2021|      2|    5|         9.0|       9.0|\n|    1000|2021|      2|    6|         9.0|       9.0|\n+--------+----+-------+-----+------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Analyze how much time is spent per ride -->  Based on date and time factors such as day of week and time of day\n",
    "\n",
    "rider_stats_by_day_hour=spark.sql (  '''SELECT rider_id, DAY(started_at) as DAY, extract(HOUR FROM started_at) as HOUR, count(*) as num_rides, AVG( DATEDIFF (second, started_at, ended_at)) * 60 as minutes_spent\n",
    "    FROM trip_fact\n",
    "    GROUP by rider_id, DAY(started_at),extract(HOUR FROM started_at)\n",
    "    Order By rider_id,DAY(started_at),extract(HOUR FROM started_at)'''\n",
    "    )\n",
    "rider_stats_by_day_hour.createOrReplaceTempView(\"rider_stats_by_day_hour\")\n",
    "spark.sql (\"Select * from rider_stats_amount limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89075010-c39f-42aa-aa48-ae7a96e3b2c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+----------+---------+------------------+\n|start_station_id|end_station_id|trip_count|timespent|          avg_time|\n+----------------+--------------+----------+---------+------------------+\n|    TA1307000121|  TA1307000044|        51|    520.0|10.196078431372548|\n|    TA1308000022|  TA1308000022|      1294|  51259.0|39.612828438948995|\n+----------------+--------------+----------+---------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# How Much Time Spent based on which station is the starting and / or ending station\n",
    "trp_stats_by_stations=spark.sql(\"select start_station_id,end_station_id,count(distinct trip_id) as trip_count,sum(trip_duration) as timespent,avg(trip_duration) as avg_time from trip_fact group by start_station_id,end_station_id\")\n",
    "trp_stats_by_stations.createOrReplaceTempView(\"trp_stats_by_stations\")\n",
    "spark.sql(\"select * from trp_stats_by_stations limit 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e202171-046c-4dc6-b2f9-7b007bdbacaf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+--------------+----------+---------+--------+\n|rider_id|start_station_id|end_station_id|trip_count|timespent|avg_time|\n+--------+----------------+--------------+----------+---------+--------+\n|    1000|    KA1504000126|  TA1309000014|         2|     35.0|    17.5|\n|    1000|    KA1504000126|  TA1309000030|         1|     43.0|    43.0|\n|    1000|    KA1504000126|  KA1504000096|         1|      9.0|     9.0|\n|    1000|    KA1504000126|  KA1504000126|         1|     45.0|    45.0|\n|    1000|    KA1504000126|         13379|         1|     29.0|    29.0|\n+--------+----------------+--------------+----------+---------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# How Much Time Spent per rider based on which station is the starting and / or ending station\n",
    "\n",
    "trp_stats_by_stations=spark.sql(\"select rider_id,start_station_id,end_station_id,count(distinct trip_id) as trip_count,sum(trip_duration) as timespent,avg(trip_duration) as avg_time from trip_fact group by rider_id,start_station_id,end_station_id\")\n",
    "trp_stats_by_stations.createOrReplaceTempView(\"trp_stats_by_rider_stations\")\n",
    "spark.sql(\"select * from trp_stats_by_rider_stations order by rider_id limit 5 \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ca9321a-eef3-43ff-9ba4-b10c621e8b76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------------+\n|rider_age|trp_count|     avg_timespent|\n+---------+---------+------------------+\n|       75|       45|              51.4|\n|       74|     1768|19.059954751131222|\n+---------+---------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Analyze how much time is spent per ride\n",
    "#Based on age of the rider at time of the ride\n",
    "rider_stats_by_age=spark.sql(\"select rider_age,count(trip_id) as trp_count,avg(trip_duration) as avg_timespent from trip_fact group by rider_age\")\n",
    "\n",
    "rider_stats_by_age.createOrReplaceTempView(\"rider_avg_stats_by_age\")\n",
    "spark.sql(\"select * from rider_avg_stats_by_age ORder By rider_age desc limit 2\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee7e7549-71e1-4328-a546-5fd748939b89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------------------+\n|Member_type|trp_count|total_timespent_hrs|\n+-----------+---------+-------------------+\n|casualrider|   918615|           326582.0|\n|     member|  3666306|          1338940.0|\n+-----------+---------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Based on whether the rider is a member or a casual rider\n",
    "rider_stats_by_member_type=spark.sql(\"select  CASE WHEN is_member=true then 'member' else 'casualrider' end as Member_type,count(trip_id) as trp_count,round(sum(trip_duration)/60 ) as total_timespent_hrs from trip_fact group by  CASE WHEN is_member=true then 'member' else 'casualrider' end \")\n",
    "rider_stats_by_member_type.createOrReplaceTempView(\"rider_stats_by_member_type\")\n",
    "spark.sql(\"select * from rider_stats_by_member_type limit 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18f447b5-8533-44bb-860c-f7f3c425bb03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---------+-----------------+\n|month|year|num_rides|    minutes_spent|\n+-----+----+---------+-----------------+\n|    7|2021|   692321|87092.90999406345|\n|    8|2021|   674409|76150.34105416742|\n+-----+----+---------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# aggregate count of rides and duration by rider, by month\n",
    "rider_stats_month_year = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT  MONTH(started_at) as month, YEAR(started_at) as year, count(*) as num_rides, AVG( DATEDIFF (second, started_at, ended_at)) * 60 as minutes_spent\n",
    "    FROM trip_fact\n",
    "    GROUP by  MONTH(started_at), YEAR(started_at);\n",
    "    \"\"\"\n",
    ")\n",
    "rider_stats_month_year.createOrReplaceTempView(\"rider_stats_month_year\")\n",
    "spark.sql(\"select * from rider_stats_month_year limit 2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac2b3fb2-0f30-4085-8b86-1b6ad6d321c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Analyze how much money is spent\n",
    "- Per month, quarter, year\n",
    "- Per member, based on the age of the rider at account start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2719a083-6762-46b8-9b77-2c8085e38f77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-------+-----+------------+----------+\n|rider_id|year|quarter|month|Total_Amount|Avg_Amount|\n+--------+----+-------+-----+------------+----------+\n|    1000|2021|      1|    2|         9.0|       9.0|\n|    1000|2021|      1|    3|         9.0|       9.0|\n|    1000|2021|      2|    4|         9.0|       9.0|\n|    1000|2021|      2|    5|         9.0|       9.0|\n|    1000|2021|      2|    6|         9.0|       9.0|\n+--------+----+-------+-----+------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#how much money is spent Per month, quarter, year Per Rider\n",
    "\n",
    "rider_stats_amount=spark.sql ('''\n",
    "           select r.rider_id,dat.year ,dat.quarter , dat.month ,sum(amount) as Total_Amount, Avg(amount) Avg_Amount\n",
    "           From\n",
    "           dim_rider r join payment_fact p on r.rider_Id=p.rider_Id\n",
    "           join date_dim dat on dat.date=p.date\n",
    "           \n",
    "           Group By r.rider_id,dat.year ,dat.quarter , dat.month\n",
    "           Order By r.rider_id,dat.year ,dat.quarter , dat.month\n",
    "           ''')\n",
    "rider_stats_amount.createOrReplaceTempView(\"rider_stats_amount\")\n",
    "spark.sql(\"select * from rider_stats_amount limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1157c107-295c-455d-8e07-62468f543fa8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+------------------+-----------------+\n|year|quarter|month|      Total_Amount|       Avg_Amount|\n+----+-------+-----+------------------+-----------------+\n|2021|      1|    2|447430.36999999965|10.01029979640691|\n|2021|      1|    3|458784.17000000004| 9.99203245126865|\n|2021|      2|    4| 471751.6599999996|9.983528241593119|\n|2021|      2|    5|485719.24999999907|9.992783961157839|\n|2021|      2|    6| 500206.6499999999|9.996136091127097|\n+----+-------+-----+------------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "money_spent=spark.sql ('''\n",
    "           select dat.year ,dat.quarter , dat.month ,sum(amount) as Total_Amount, Avg(amount) Avg_Amount\n",
    "           From\n",
    "           dim_rider r join payment_fact p on r.rider_Id=p.rider_Id\n",
    "           join date_dim dat on dat.date=p.date\n",
    "           \n",
    "           Group By dat.year ,dat.quarter , dat.month\n",
    "           Order By dat.year ,dat.quarter , dat.month\n",
    "           ''')\n",
    "money_spent.createOrReplaceTempView(\"money_spent\")\n",
    "spark.sql(\"select * from money_spent limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1ebc5b5-dad6-4aff-a3dc-14e648a7741d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n|rider_age|Total_Amt|\n+---------+---------+\n|       29| 438219.0|\n|       26| 470439.0|\n|       65|   8469.0|\n|       19| 456030.0|\n+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#Per member, based on the age of the rider at account start\n",
    "\n",
    "rider_stats_by_age=spark.sql('''select floor (months_between (account_start_Date, birthday) / 12) as rider_age,sum(p.amount) as Total_Amt   From\n",
    "           dim_rider r join payment_fact p on r.rider_Id=p.rider_Id\n",
    "           where is_member=true\n",
    "          Group By floor (months_between (account_start_Date, birthday) / 12)\n",
    "                     ''')\n",
    "rider_stats_by_age.createOrReplaceTempView(\"rider_stats_by_age\")\n",
    "spark.sql(\"select * from rider_stats_by_age limit 4\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0426dbec-d77c-401b-a449-7646d8c463f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----------+--------------+-----------------+\n|rider_id|Month|trip_count|total_trip_mns|avg_monthly_rides|\n+--------+-----+----------+--------------+-----------------+\n|    1000|    4|         2|          34.0|              2.0|\n|    1000|   11|         3|          58.0|              3.0|\n|    1003|    6|        18|         296.0|             18.0|\n|    1003|   10|        16|         190.0|             16.0|\n+--------+-----+----------+--------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# how many minutes the rider spends on a bike per month\n",
    "spark.sql ('''\n",
    "           select rider_id,MONTH(started_at) as Month,count(distinct trip_id) as trip_count, sum(trip_duration) as total_trip_mns \n",
    "           ,AVG(COUNT(DISTINCT trip_id)) OVER (PARTITION BY rider_id, MONTH(started_at)) AS avg_monthly_rides\n",
    "             from trip_fact \n",
    "           Group by rider_id,MONTH(started_at)\n",
    "           limit 4\n",
    "           ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a20b4eab-7b1a-4045-9278-60a2532fbd55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# how many minutes the rider spends on a bike per month\n",
    "spark.sql ('''\n",
    "           select rider_id,MONTH(started_at) as Month,count(distinct trip_id) as trip_count, sum(trip_duration) as total_trip_mns \n",
    "           ,AVG(COUNT(DISTINCT trip_id)) OVER (PARTITION BY rider_id, MONTH(started_at)) AS avg_monthly_rides\n",
    "             from trip_fact t join Dim_re\n",
    "           Group by rider_id,MONTH(started_at)\n",
    "           limit 4\n",
    "           ''').show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "BikeShare_DataLake",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
