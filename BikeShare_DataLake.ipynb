{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4714ded0-6c66-4c5f-8b35-94902b7e35ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.functions import isnan, count, when, col, desc, udf, col, sort_array, asc, avg\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05e65d29-345d-4126-8e74-bc93c55eb2d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Creat Spark Session\n",
    "spark=SparkSession.builder.appName(\"DW_BikeShare\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2e6895a-548f-4e9b-82c4-d50a34177a20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create Filestore paths\n",
    "payments_file_path=\"/FileStore/payments.csv\"\n",
    "rider_file_path=\"/FileStore/riders.csv\"\n",
    "stations_file_path=\"/FileStore/stations.csv\"\n",
    "trips_file_path=\"/FileStore/trips.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61a30b87-44d1-4be4-aab5-830b64b84e48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create payment dataframe from csv file\n",
    "df_pay=spark.read.csv(payments_file_path,header=False,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38073810-d191-4495-9b15-4d3d3a2af6d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create rider dataframe from csv file\n",
    "df_rider=spark.read.csv(rider_file_path,header=False,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3496e72d-9998-4ad5-9a24-7fb77dd3e375",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create stations dataframe from csv file\n",
    "df_stations=spark.read.csv(stations_file_path,header=False,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd080792-e14f-4af1-8ddc-151a4bf406d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create trips dataframe from csv file\n",
    "df_trips=spark.read.csv(trips_file_path,header=False,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b16eb2cc-99df-423a-937a-0f18e7e66cd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Add header record to the dataframe\n",
    "df_riders=df_rider.toDF(\"rider_id\",\"first\",\"last\",\"address\",\"birthday\",\"account_start_date\",\"account_end_date\",\"is_member\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e85bd249-f106-457b-b6fd-b63acaf45c0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- rider_id: integer (nullable = true)\n |-- first: string (nullable = true)\n |-- last: string (nullable = true)\n |-- address: string (nullable = true)\n |-- birthday: date (nullable = true)\n |-- account_start_date: date (nullable = true)\n |-- account_end_date: date (nullable = true)\n |-- is_member: boolean (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_riders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10f43812-adcd-4b8b-9959-3a5d20d7f142",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[rider_id: int, first: string, last: string, address: string, birthday: date, account_start_date: date, account_end_date: date, is_member: boolean]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_riders.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43ee54bd-b8a3-4490-8161-7de158726f17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Add header record to the dataframe\n",
    "df_trip=df_trips.toDF(\"trip_id\",\"rideable_type\",\"started_at\",\"ended_at\",\"start_station_id\",\"end_station_id\",\"rider_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf420c38-d841-4c4a-8d63-71e46fb3fbbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- trip_id: string (nullable = true)\n |-- rideable_type: string (nullable = true)\n |-- started_at: timestamp (nullable = true)\n |-- ended_at: timestamp (nullable = true)\n |-- start_station_id: string (nullable = true)\n |-- end_station_id: string (nullable = true)\n |-- rider_id: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_trip.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20f052a9-3947-4683-baa5-0cdf1b20e6a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[trip_id: string, rideable_type: string, started_at: timestamp, ended_at: timestamp, start_station_id: string, end_station_id: string, rider_id: int]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trip.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9732d1c9-6130-4958-9605-417c649b45a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Add header record to the dataframe\n",
    "df_station=df_stations.toDF(\"station_id\",\"name\",\"latitude\",\"longitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fb5c178-3cc2-4d41-8eff-adca2f348344",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[station_id: string, name: string, latitude: double, longitude: double]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop Duplicates\n",
    "df_station.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da56091-02ed-4f99-afe7-d999f2dbabb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- station_id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_station.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c610afb-c349-4f83-8360-227b4548c2d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Add header record to the dataframe\n",
    "df_payment=df_pay.toDF(\"id\",\"date\",\"amount\",\"rider_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d3198f2-ef51-41f6-a668-0bb03b40182c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+--------+\n| id|      date|amount|rider_id|\n+---+----------+------+--------+\n|  1|2019-05-01|   9.0|    1000|\n|  2|2019-06-01|   9.0|    1000|\n|  3|2019-07-01|   9.0|    1000|\n|  4|2019-08-01|   9.0|    1000|\n|  5|2019-09-01|   9.0|    1000|\n+---+----------+------+--------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_payment.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69e155d6-c624-40b8-b63c-64f981131cdc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[id: int, date: date, amount: double, rider_id: int]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_payment.printSchema()\n",
    "df_payment.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3c4bceb-bed1-4a71-9814-94036e1ea532",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df_payment.count()\n",
    "#1946607\n",
    "#Create Delta file for Payment\n",
    "df_payment.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/bronze_payment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1024b3d4-7584-462b-93e7-e06be299fc73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+--------+\n| id|      date|amount|rider_id|\n+---+----------+------+--------+\n|  1|2019-05-01|   9.0|    1000|\n|  2|2019-06-01|   9.0|    1000|\n|  3|2019-07-01|   9.0|    1000|\n|  4|2019-08-01|   9.0|    1000|\n|  5|2019-09-01|   9.0|    1000|\n+---+----------+------+--------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_payment.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c45386f5-1b97-4621-8277-0e3b2fc1f5e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_p=spark.read.format(\"delta\").load(\"/delta/bronze_payment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3892ffcc-f4f1-4636-a281-a0c7abf75012",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(id=1064462, date=datetime.date(2020, 6, 1), amount=9.0, rider_id=42106),\n",
       " Row(id=1064463, date=datetime.date(2020, 7, 1), amount=9.0, rider_id=42106),\n",
       " Row(id=1064464, date=datetime.date(2020, 8, 1), amount=9.0, rider_id=42106),\n",
       " Row(id=1064465, date=datetime.date(2020, 9, 1), amount=9.0, rider_id=42106),\n",
       " Row(id=1064466, date=datetime.date(2020, 10, 1), amount=9.0, rider_id=42106)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dd19512-6921-46e8-aa40-671ca800aa71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Create Delta table for Payment\n",
    "spark.sql (\"CREATE TABLE IF NOT EXISTS silver_payment USING DELTA LOCATION '/delta/bronze_payment' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "211beb42-30db-4049-ba16-b5103af6da40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Delta table for rider\n",
    "df_riders.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/bronze_rider\")\n",
    "spark.sql (\"CREATE TABLE IF NOT EXISTS silver_rider USING DELTA LOCATION '/delta/bronze_rider' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c3a24de-0dfa-472e-9cad-ce9ccc0f9153",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Delta table for trips data\n",
    "df_trip.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/bronze_trip\")\n",
    "spark.sql (\"CREATE TABLE IF NOT EXISTS silver_trip USING DELTA LOCATION '/delta/bronze_trip' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1062b9f7-d31e-486d-90f9-77cedfeeb903",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Delta table for Stations data\n",
    "df_station.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/bronze_station\")\n",
    "spark.sql (\"CREATE TABLE IF NOT EXISTS silver_station USING DELTA LOCATION '/delta/bronze_station' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8bd4bae-cc1e-4336-85fc-1832c6c1adde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F \n",
    "min_date=spark.sql(\"select date(min(date)) from silver_payment\").collect()[0][0]\n",
    "#print(min_date)\n",
    "max_date=spark.sql(\"select date(max(date)) from silver_payment\").collect()[0][0]\n",
    "#print(max_date)\n",
    "date_dim = spark.range(0, (max_date - min_date).days + 1) \\\n",
    "    .withColumn(\"date\", F.date_add(F.lit(min_date), F.col(\"id\").cast(\"integer\"))) \\\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"date\").cast(\"integer\")) \\\n",
    "    .withColumn(\"day_of_month\", F.dayofmonth(\"date\").cast(\"integer\")) \\\n",
    "    .withColumn(\"day_of_year\", F.dayofyear(\"date\").cast(\"integer\")) \\\n",
    "    .withColumn(\"week_of_year\", F.weekofyear(\"date\").cast(\"integer\")) \\\n",
    "    .withColumn(\"month\", F.month(\"date\").cast(\"integer\")) \\\n",
    "    .withColumn(\"month_name\", F.monthname(\"date\")) \\\n",
    "    .withColumn(\"quarter\", F.quarter(\"date\").cast(\"integer\")) \\\n",
    "    .withColumn(\"year\", F.year(\"date\").cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ac8bfe8-9848-4fd4-b807-f4267990eba8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "date_dim.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"date_dim\")\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a276b3e-cfae-4b80-b65d-c49bcb02c22b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Payment Fact table \n",
    "spark.sql('''CREATE TABLE IF NOT EXISTS payment_fact USING DELTA AS \n",
    "          SELECT DISTINCT p.id AS payment_id, dd.id as date_id,amount,rider_id FROM silver_payment p\n",
    "          join date_dim dd on dd.date=p.date\n",
    "          ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7b0c878-b0f9-46a9-b797-3c62c7f43f49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+--------+\n|payment_id|date_id|amount|rider_id|\n+----------+-------+------+--------+\n|       264|   2038|   9.0|    1011|\n|       516|   2922| 17.99|    1021|\n+----------+-------+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select * from payment_fact limit 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50e73758-8f96-4f26-809b-a224fc9a7461",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Create Dim Rider \n",
    "spark.sql('''CREATE TABLE IF NOT EXISTS dim_rider USING DELTA AS \n",
    "          SELECT DISTINCT * FROM silver_rider''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30f92093-d52b-477f-b26c-5de0eced020c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+---------+--------------------+----------+------------------+----------------+---------+\n|rider_id|  first|     last|             address|  birthday|account_start_date|account_end_date|is_member|\n+--------+-------+---------+--------------------+----------+------------------+----------------+---------+\n|    1040|Matthew|    Watts|   323 Matthew Flats|1996-12-24|        2014-11-26|            NULL|     true|\n|    1065|Rebecca|Mccormick|4912 Smith Alley ...|1983-07-18|        2017-07-13|            NULL|    false|\n+--------+-------+---------+--------------------+----------+------------------+----------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from dim_rider  limit 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c8948cc-0069-4a86-a1d6-682fbdec942b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Create DIM_Station\n",
    "spark.sql('''CREATE TABLE IF NOT EXISTS dim_station USING DELTA AS \n",
    "          SELECT DISTINCT  *  FROM silver_station''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "348c4960-38cd-4ef2-abdb-60da15affce7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------------+------------------+\n|  station_id|                name|         latitude|         longitude|\n+------------+--------------------+-----------------+------------------+\n|       15550|Canal St & Taylor St|        41.870257|-87.63947399999999|\n|TA1307000150|Pine Grove Ave & ...|41.94947274088333|-87.64645278453827|\n+------------+--------------------+-----------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from dim_station limit 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a81fea09-6685-4a90-b40a-463fff80227a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Dim Ridetype table\n",
    "spark.sql ('''\n",
    "           CREATE TABLE IF NOT EXISTS dim_ride_type\n",
    "USING DELTA\n",
    "AS\n",
    "SELECT\n",
    "  ROW_NUMBER() OVER (ORDER BY rideable_type) AS ride_type_id,\n",
    "  rideable_type\n",
    "FROM\n",
    "  silver_trip\n",
    "GROUP BY\n",
    "  rideable_type\n",
    "  '''\n",
    "           \n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bd347a6-34c8-4249-b725-60374506518c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n|ride_type_id|rideable_type|\n+------------+-------------+\n|           1| classic_bike|\n|           2|  docked_bike|\n|           3|electric_bike|\n+------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql (\"select * from dim_ride_type\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a7273eb-ec33-452f-8237-49bfd77f4472",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+--------+\n|payment_id|date_id|amount|rider_id|\n+----------+-------+------+--------+\n|    856454|   2891|   9.0|   34062|\n|    856453|   2860|   9.0|   34062|\n+----------+-------+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql (\"select * from payment_fact where rider_id=34062\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32a8653d-b5c9-4b58-b51c-2131ff98d7d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Create Trip Fact Table\n",
    "spark.sql ('''\n",
    "           CREATE TABLE IF NOT EXISTS trip_fact USING DELTA AS \n",
    "\n",
    "          select t.trip_id ,t.rider_id,t.start_station_id,t.end_station_id,rt.ride_type_id,sdd.id as start_date_id,edd.id as end_date_id, started_at,ended_at,\n",
    "           round(datediff (second, t.started_at, t.ended_at) / 60)  AS trip_duration,\n",
    "            floor (months_between (t.started_at, r.birthday) / 12) as rider_age,r.is_member\n",
    "from silver_trip t\n",
    "join dim_rider r on t.rider_id=r.rider_id\n",
    "join dim_station s on s.station_id=t.start_station_id\n",
    "join dim_station se on se.station_id=t.end_station_id\n",
    "join dim_ride_type rt on rt.rideable_type=t.rideable_type\n",
    "join date_dim sdd on sdd.date=date(t.started_at)\n",
    "join date_dim edd on edd.date=date(t.ended_at)\n",
    "\n",
    "'''\n",
    "           \n",
    "           \n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e4044bd-87e6-400e-8c2a-951c51061227",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+----------------+--------------+------------+-------------+-----------+-------------------+-------------------+-------------+---------+---------+\n|         trip_id|rider_id|start_station_id|end_station_id|ride_type_id|start_date_id|end_date_id|         started_at|           ended_at|trip_duration|rider_age|is_member|\n+----------------+--------+----------------+--------------+------------+-------------+-----------+-------------------+-------------------+-------------+---------+---------+\n|222BB8E5059252D7|   34062|    KA1503000064|         13021|           1|         3054|       3054|2021-06-13 09:48:47|2021-06-13 10:07:23|         19.0|       30|     true|\n|1826E16CB5486018|    5342|    TA1306000010|         13021|           1|         3062|       3062|2021-06-21 22:59:13|2021-06-21 23:04:29|          5.0|       26|     true|\n+----------------+--------+----------------+--------------+------------+-------------+-----------+-------------------+-------------------+-------------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql (\"select * from trip_fact limit 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89ccb1fb-2bd7-49d0-b64d-5ed2fef446b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n|           min_date|           max_Date|\n+-------------------+-------------------+\n|2021-02-01 01:07:04|2022-02-01 00:12:04|\n+-------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql (\"select min(started_at) as min_date,max(ended_at) as max_Date from trip_fact limit 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96027540-f5c2-4df5-991b-cc8dae3d6188",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----------+------------+-----------+------------+-----+----------+-------+----+\n|  id|      date|day_of_week|day_of_month|day_of_year|week_of_year|month|month_name|quarter|year|\n+----+----------+-----------+------------+-----------+------------+-----+----------+-------+----+\n|2466|2019-11-03|          1|           3|        307|          44|   11|       Nov|      4|2019|\n|2467|2019-11-04|          2|           4|        308|          45|   11|       Nov|      4|2019|\n|2468|2019-11-05|          3|           5|        309|          45|   11|       Nov|      4|2019|\n|2469|2019-11-06|          4|           6|        310|          45|   11|       Nov|      4|2019|\n|2470|2019-11-07|          5|           7|        311|          45|   11|       Nov|      4|2019|\n+----+----------+-----------+------------+-----------+------------+-----+----------+-------+----+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from date_dim limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a89abdb1-1cff-452e-9599-1880c2dab11e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Data Anaylysis  how much time is spent per ride Based on the following Factors:\n",
    "- Based on date and time factors such as day of week and time of day\n",
    "- Based on which station is the starting and / or ending station\n",
    "- Based on age of the rider at time of the ride\n",
    "- Based on whether the rider is a member or a casual rider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f2670b1-d639-429b-836f-3e0abeff6dd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----+---------+-------------+\n|rider_id|DAY|HOUR|num_rides|minutes_spent|\n+--------+---+----+---------+-------------+\n|    1000|  2|  10|        1|      95640.0|\n|    1000|  2|  17|        1|      53880.0|\n|    1000|  2|  23|        1|      31620.0|\n|    1000|  3|  14|        1|      84660.0|\n|    1000|  6|  19|        1|     170820.0|\n+--------+---+----+---------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Analyze how much time is spent per ride -->  Based on date and time factors such as day of week and time of day\n",
    "\n",
    "rider_stats_by_day_hour=spark.sql (  '''SELECT rider_id, DAY(started_at) as DAY, extract(HOUR FROM started_at) as HOUR, count(*) as num_rides, AVG( DATEDIFF (second, started_at, ended_at)) * 60 as minutes_spent\n",
    "    FROM trip_fact\n",
    "    GROUP by rider_id, DAY(started_at),extract(HOUR FROM started_at)\n",
    "    Order By rider_id,DAY(started_at),extract(HOUR FROM started_at)'''\n",
    "    )\n",
    "rider_stats_by_day_hour.createOrReplaceTempView(\"rider_stats_by_day_hour\")\n",
    "spark.sql (\"Select * from rider_stats_by_day_hour limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89075010-c39f-42aa-aa48-ae7a96e3b2c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+----------+---------+------------------+\n|start_station_id|end_station_id|trip_count|timespent|          avg_time|\n+----------------+--------------+----------+---------+------------------+\n|             600|           600|       114|  37575.0|329.60526315789474|\n|    TA1308000022|  TA1308000022|      1294|  51259.0|39.612828438948995|\n+----------------+--------------+----------+---------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# How Much Time Spent based on which station is the starting and / or ending station\n",
    "trp_stats_by_stations=spark.sql(\"select start_station_id,end_station_id,count(distinct trip_id) as trip_count,sum(trip_duration) as timespent,avg(trip_duration) as avg_time from trip_fact group by start_station_id,end_station_id\")\n",
    "trp_stats_by_stations.createOrReplaceTempView(\"trp_stats_by_stations\")\n",
    "spark.sql(\"select * from trp_stats_by_stations limit 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e202171-046c-4dc6-b2f9-7b007bdbacaf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+--------------+----------+---------+--------+\n|rider_id|start_station_id|end_station_id|trip_count|timespent|avg_time|\n+--------+----------------+--------------+----------+---------+--------+\n|    1000|    KA1504000126|  TA1309000014|         2|     35.0|    17.5|\n|    1000|    KA1504000126|  TA1309000030|         1|     43.0|    43.0|\n|    1000|    KA1504000126|  KA1504000096|         1|      9.0|     9.0|\n|    1000|    KA1504000126|  KA1504000126|         1|     45.0|    45.0|\n|    1000|    KA1504000126|         13379|         1|     29.0|    29.0|\n+--------+----------------+--------------+----------+---------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# How Much Time Spent per rider based on which station is the starting and / or ending station\n",
    "\n",
    "trp_stats_by_stations=spark.sql(\"select rider_id,start_station_id,end_station_id,count(distinct trip_id) as trip_count,sum(trip_duration) as timespent,avg(trip_duration) as avg_time from trip_fact group by rider_id,start_station_id,end_station_id\")\n",
    "trp_stats_by_stations.createOrReplaceTempView(\"trp_stats_by_rider_stations\")\n",
    "spark.sql(\"select * from trp_stats_by_rider_stations order by rider_id limit 5 \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ca9321a-eef3-43ff-9ba4-b10c621e8b76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------------+\n|rider_age|trp_count|     avg_timespent|\n+---------+---------+------------------+\n|       75|       45|              51.4|\n|       74|     1768|19.059954751131222|\n+---------+---------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Analyze how much time is spent per ride\n",
    "#Based on age of the rider at time of the ride\n",
    "rider_stats_by_age=spark.sql(\"select rider_age,count(trip_id) as trp_count,avg(trip_duration) as avg_timespent from trip_fact group by rider_age\")\n",
    "\n",
    "rider_stats_by_age.createOrReplaceTempView(\"rider_avg_stats_by_age\")\n",
    "spark.sql(\"select * from rider_avg_stats_by_age ORder By rider_age desc limit 2\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee7e7549-71e1-4328-a546-5fd748939b89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------------------+\n|Member_type|trp_count|total_timespent_hrs|\n+-----------+---------+-------------------+\n|casualrider|   918615|           326582.0|\n|     member|  3666306|          1338940.0|\n+-----------+---------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Based on whether the rider is a member or a casual rider\n",
    "rider_stats_by_member_type=spark.sql(\"select  CASE WHEN is_member=true then 'member' else 'casualrider' end as Member_type,count(trip_id) as trp_count,round(sum(trip_duration)/60 ) as total_timespent_hrs from trip_fact group by  CASE WHEN is_member=true then 'member' else 'casualrider' end \")\n",
    "rider_stats_by_member_type.createOrReplaceTempView(\"rider_stats_by_member_type\")\n",
    "spark.sql(\"select * from rider_stats_by_member_type limit 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18f447b5-8533-44bb-860c-f7f3c425bb03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---------+-----------------+\n|month|year|num_rides|    minutes_spent|\n+-----+----+---------+-----------------+\n|    4|2021|   298207|86494.35231231997|\n|    5|2021|   450994|95410.06802751256|\n+-----+----+---------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# aggregate count of rides and duration by rider, by month\n",
    "rider_stats_month_year = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT  MONTH(started_at) as month, YEAR(started_at) as year, count(*) as num_rides, AVG( DATEDIFF (second, started_at, ended_at)) * 60 as minutes_spent\n",
    "    FROM trip_fact\n",
    "    GROUP by  MONTH(started_at), YEAR(started_at);\n",
    "    \"\"\"\n",
    ")\n",
    "rider_stats_month_year.createOrReplaceTempView(\"rider_stats_month_year\")\n",
    "spark.sql(\"select * from rider_stats_month_year limit 2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac2b3fb2-0f30-4085-8b86-1b6ad6d321c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Analyze how much money is spent\n",
    "- Per month, quarter, year\n",
    "- Per member, based on the age of the rider at account start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2719a083-6762-46b8-9b77-2c8085e38f77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-------+-----+------------+----------+\n|rider_id|year|quarter|month|Total_Amount|Avg_Amount|\n+--------+----+-------+-----+------------+----------+\n|    1000|2019|      2|    5|         9.0|       9.0|\n|    1000|2019|      2|    6|         9.0|       9.0|\n|    1000|2019|      3|    7|         9.0|       9.0|\n|    1000|2019|      3|    8|         9.0|       9.0|\n|    1000|2019|      3|    9|         9.0|       9.0|\n+--------+----+-------+-----+------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#how much money is spent Per month, quarter, year Per Rider\n",
    "\n",
    "rider_stats_amount=spark.sql ('''\n",
    "           select r.rider_id,dat.year ,dat.quarter , dat.month ,sum(amount) as Total_Amount, Avg(amount) Avg_Amount\n",
    "           From\n",
    "           dim_rider r join payment_fact p on r.rider_Id=p.rider_Id\n",
    "           join date_dim dat on dat.Id=p.date_id\n",
    "           \n",
    "           Group By r.rider_id,dat.year ,dat.quarter , dat.month\n",
    "           Order By r.rider_id,dat.year ,dat.quarter , dat.month\n",
    "           ''')\n",
    "rider_stats_amount.createOrReplaceTempView(\"rider_stats_amount\")\n",
    "spark.sql(\"select * from rider_stats_amount limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1157c107-295c-455d-8e07-62468f543fa8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+------------------+------------------+\n|year|quarter|month|      Total_Amount|        Avg_Amount|\n+----+-------+-----+------------------+------------------+\n|2013|      1|    2|              12.9|              12.9|\n|2013|      1|    3| 817.7499999999999|10.095679012345677|\n|2013|      2|    4|1672.6500000000003|  9.89733727810651|\n|2013|      2|    5|           2716.71|10.290568181818182|\n|2013|      2|    6|3775.2999999999997|10.148655913978494|\n+----+-------+-----+------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "money_spent=spark.sql ('''\n",
    "           select dat.year ,dat.quarter , dat.month ,sum(amount) as Total_Amount, Avg(amount) Avg_Amount\n",
    "           From\n",
    "           dim_rider r join payment_fact p on r.rider_Id=p.rider_Id\n",
    "           join date_dim dat on dat.id=p.date_id\n",
    "           \n",
    "           Group By dat.year ,dat.quarter , dat.month\n",
    "           Order By dat.year ,dat.quarter , dat.month\n",
    "           ''')\n",
    "money_spent.createOrReplaceTempView(\"money_spent\")\n",
    "spark.sql(\"select * from money_spent limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1ebc5b5-dad6-4aff-a3dc-14e648a7741d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n|rider_age|Total_Amt|\n+---------+---------+\n|       29| 438219.0|\n|       26| 470439.0|\n|       65|   8469.0|\n|       54|  58365.0|\n+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#Per member, based on the age of the rider at account start\n",
    "\n",
    "rider_stats_by_age=spark.sql('''select floor (months_between (account_start_Date, birthday) / 12) as rider_age,sum(p.amount) as Total_Amt   From\n",
    "           dim_rider r join payment_fact p on r.rider_Id=p.rider_Id\n",
    "           where is_member=true\n",
    "          Group By floor (months_between (account_start_Date, birthday) / 12)\n",
    "                     ''')\n",
    "rider_stats_by_age.createOrReplaceTempView(\"rider_stats_by_age\")\n",
    "spark.sql(\"select * from rider_stats_by_age limit 4\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0426dbec-d77c-401b-a449-7646d8c463f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----------+--------------+-----------------+\n|rider_id|Month|trip_count|total_trip_mns|avg_monthly_rides|\n+--------+-----+----------+--------------+-----------------+\n|    1000|    4|         2|          34.0|              2.0|\n|    1000|   11|         3|          58.0|              3.0|\n|    1003|    6|        18|         296.0|             18.0|\n|    1003|   10|        16|         190.0|             16.0|\n+--------+-----+----------+--------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# how many minutes the rider spends on a bike per month\n",
    "spark.sql ('''\n",
    "           select rider_id,MONTH(started_at) as Month,count(distinct trip_id) as trip_count, sum(trip_duration) as total_trip_mns \n",
    "           ,AVG(COUNT(DISTINCT trip_id)) OVER (PARTITION BY rider_id, MONTH(started_at)) AS avg_monthly_rides\n",
    "             from trip_fact \n",
    "           Group by rider_id,MONTH(started_at)\n",
    "           limit 4\n",
    "           ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a20b4eab-7b1a-4045-9278-60a2532fbd55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----------+--------------+-----------------+\n|rider_id|Month|trip_count|total_trip_mns|avg_monthly_rides|\n+--------+-----+----------+--------------+-----------------+\n|    1000|    4|         2|          34.0|              2.0|\n|    1000|   11|         3|          58.0|              3.0|\n|    1003|    6|        18|         296.0|             18.0|\n|    1003|   10|        16|         190.0|             16.0|\n+--------+-----+----------+--------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# how many minutes the rider spends on a bike per month\n",
    "spark.sql ('''\n",
    "           select t.rider_id,MONTH(started_at) as Month,count(distinct trip_id) as trip_count, sum(trip_duration) as total_trip_mns \n",
    "           ,AVG(COUNT(DISTINCT trip_id)) OVER (PARTITION BY t.rider_id, MONTH(started_at)) AS avg_monthly_rides\n",
    "             from trip_fact t join Dim_rider d on d.rider_id=t.rider_id\n",
    "           Group by t.rider_id,MONTH(started_at)\n",
    "           limit 4\n",
    "           ''').show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1302250297057711,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "BikeShare_DataLake",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
